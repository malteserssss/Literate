import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import csv
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')


def get_date(job_date_text):
    """Extract the date when the job was posted."""
    job_date_text = job_date_text.replace('Added on ', '')
    if 'Today' in job_date_text:
        return datetime.now().date()
    elif 'Yesterday' in job_date_text:
        return datetime.now().date() - timedelta(1)
    else:
        return datetime.strptime(job_date_text, "%d %b").replace(year=datetime.now().year).date()


def get_soup(url):
    """Send a GET request and parse the page's content."""
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:99.0) Gecko/20100101 Firefox/99.0"}
    response = requests.get(url, headers=headers)
    response.raise_for_status()  # Raise an HTTPError if the HTTP request returned an unsuccessful status code.
    soup = BeautifulSoup(response.content, "html.parser")
    return soup


def get_jobs(soup):
    """Extract the job posts on the page."""
    job_posts = soup.find_all('article', class_='job-post job-normal-post')

    jobs = []
    for job_post in job_posts:
        job = {}

        # If there is no 'foreign' in class, job is local
        if 'foreign' not in job_post["class"]:
            job_title = job_post.find("h2").text.strip()

            job_date_text = job_post.find("time", class_="jsx-3049076512").text
            job_date = get_date(job_date_text)

            # Skip to next job post if this job is older than 1 week
            if job_date < datetime.now().date() - timedelta(7):
                continue

            job["job_title"] = job_title
            job["job_date"] = job_date

            jobs.append(job)

    return jobs


def write_to_csv(jobs):
    """Write the jobs to a CSV file."""
    # Make sure there's at least one job before writing to CSV.
    if jobs:
        keys = jobs[0].keys()
        with open('jobs.csv', 'w', newline='', encoding='utf-8') as output_file:
            dict_writer = csv.DictWriter(output_file, keys)
            dict_writer.writeheader()
            dict_writer.writerows(jobs)
    else:
        logging.info("No jobs found.")


def jobs_scraper():
    """Scrap the website and gather the job posts."""
    url = 'https://www.bestjobs.eu/ro/locuri-de-munca?keyword=IT&location=Bucharest'
    soup = get_soup(url)
    jobs = get_jobs(soup)
    write_to_csv(jobs)


jobs_scraper()
